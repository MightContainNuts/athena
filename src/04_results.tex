%! Author = deandidion
%! Date = 09.07.25

% Preamble

\chapter{Results}

The initial investigation into AI-supported impact measurement yielded several key findings:

\begin{enumerate}

 \item \textbf{Lack of Standardization:} Most organizations currently rely on ad hoc methods to assess impact, often combining manual approaches (e.g., interviews, surveys) with limited automation. The absence of standardized frameworks hampers comparability and reproducibility.

 \item \textbf{Interest in AI for Qualitative Data:} Stakeholders expressed significant interest in using Artificial Intelligence—particularly Natural Language Processing (NLP)—to process qualitative feedback. Use cases include sentiment analysis, thematic clustering, and the summarization of interviews or open-text survey responses.


\item \textbf{Fragmented Tool Landscape:} Existing solutions are highly fragmented, ranging from simple spreadsheets to customized dashboards. While some experimentation with AI tools such as ChatGPT, ReadAI, or MonkeyLearn is underway, these efforts are typically isolated and in early stages.

\item \textbf{Trust and Transparency Concerns:} Ethical concerns—especially regarding the interpretability and fairness of AI models—remain a major barrier to adoption. Respondents emphasized the importance of transparency, particularly when impact data is used to justify funding or policy decisions.

\item \textbf{AI as an Analytical Support Tool:} The most promising use of AI is in post-processing rather than decision-making. Participants highlighted the utility of AI for accelerating repetitive tasks such as summarizing text, identifying patterns, and detecting anomalies, while retaining human oversight for final evaluations.


\end{enumerate}
