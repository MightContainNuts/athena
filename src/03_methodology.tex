\chapter{Methodology}\label{ch:methodology}

This chapter outlines the methodology guiding this research.
Building on the principles of \textbf{Design Science Research (DSR)}, it describes the process through which an AI-enabled Impact Measurement and Management (IMM) artefact was designed, developed, demonstrated, and evaluated within the context of \textit{Inluma} and the Public Value Hub in Leipzig.
The chapter first introduces the methodological foundation, then explains the research context, followed by the stages of artefact creation and evaluation, and concludes with reflections on contributions and ethical considerations.

% ===========================
% SECTION 3.1
% ===========================
\section{Research Methodology}\label{sec:research-methodology}

This research applies the \textbf{Design Science Research (DSR)} methodology, which provides a structured process for developing and evaluating innovative artefacts in information systems research~\parencite{hevner2004design, peffers2007design}.
DSR is particularly suited to this thesis, as the objective is not only to analyze existing IMM practices but to design, implement, and evaluate a novel artefact that integrates Artificial Intelligence (AI) into impact measurement and management.

The artefact is implemented as a \textbf{prototypical instantiation}—a proof of concept designed to explore feasibility and generate insights for future development.
The evaluation therefore focuses on usability, interpretability, and improvement potential rather than generalizability or market readiness.

Following the DSR framework, the research proceeds through six iterative stages (Figure~\ref{fig:dsr-cycle}): problem identification, knowledge base grounding, artefact design and development, demonstration, evaluation, and reflection and contribution.

% ===========================
% DSR process cycle figure
% ===========================
\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        node distance=2.5cm,
        every node/.style={font=\sffamily, align=center},
        box/.style={rectangle, rounded corners, draw=black, fill=gray!10, minimum width=3.8cm, minimum height=1cm},
        arrow/.style={-{Stealth[length=3mm,width=2mm]}, thick}
    ]

    % Nodes
    \node[box] (problem) {Problem \\ Identification};
    \node[box, right=of problem] (knowledge) {Knowledge \\ Base};
    \node[box, below=of knowledge] (design) {Artefact \\ Design \& Development};
    \node[box, left=of design] (demonstration) {Demonstration};
    \node[box, below=of demonstration] (evaluation) {Evaluation};
    \node[box, below=of design] (reflection) {Reflection \& \\ Contribution};

    % Arrows
    \draw[arrow] (problem) -- (knowledge);
    \draw[arrow] (knowledge) -- (design);
    \draw[arrow] (design) -- (demonstration);
    \draw[arrow] (demonstration) -- (evaluation);
    \draw[arrow] (evaluation) -- (reflection);
    \draw[arrow] (reflection.west) .. controls +(-2,0) and +(-2,0) .. (problem.west);

    \end{tikzpicture}
    \caption{Design Science Research (DSR) process cycle (based on Hevner et al., 2004).}
    \label{fig:dsr-cycle}
\end{figure}


% ===========================
% SECTION 3.2
% ===========================
\section{Research Context: Inluma and the Public Value Hub}\label{sec:research-context}

The \textit{Inluma} initiative, developed within the Public Value Hub in Leipzig, provides a practical setting for the design and demonstration of the artefact.
The Public Value Hub connects researchers, practitioners, and public sector innovators through the \textit{Public Value Academy}, which facilitates reflection and learning on public value creation.
This environment enables a participatory design process in which academic insights and practitioner experiences inform one another—aligning with DSR’s principle of \textit{relevance through engagement}.

\textit{Inluma} functions as both a conceptual framework and a digital platform for exploring AI-supported learning and reflection processes.
It is therefore well suited for the iterative development and evaluation of a proof-of-concept artefact within a real-world innovation ecosystem.


% ===========================
% SECTION 3.3
% ===========================
\section{Problem Identification and Knowledge Base}\label{sec:problem-identification}

The first stages of the DSR process involve identifying the practical problem and grounding it in a solid theoretical and empirical knowledge base.
In this research, qualitative inquiry was employed to understand existing challenges in impact measurement and management and to identify opportunities for AI integration.

Semi-structured interviews and participatory workshops were conducted with public sector innovators and researchers affiliated with the Public Value Hub and the Public Value Academy.
These engagements focused on:
\begin{itemize}
    \item Limitations in current impact measurement and reporting practices,
    \item Approaches to operationalizing concepts such as \textbf{public value} and \textbf{social impact},
    \item Stakeholder needs for learning, reflection, and transparency in evaluation processes.
\end{itemize}

A thematic analysis of the qualitative data informed the artefact’s design requirements.
Key insights emphasized the need for interpretability, adaptability, and the ability to integrate both quantitative and narrative dimensions of impact.
The theoretical grounding draws on literature from impact measurement, artificial intelligence, and public sector innovation, providing the knowledge base that guides artefact development.


% ===========================
% SECTION 3.4
% ===========================
\section{Artefact Design and Development}\label{sec:artefact-design}

The central outcome of the DSR process is the design and development of an artefact that addresses the identified problem.
In this case, the artefact is an \textbf{AI-enabled Impact Measurement and Management (IMM) framework} instantiated within the \textit{Inluma} environment.
It aims to support sense-making in impact assessment through natural language processing (NLP), semantic search, and automated knowledge organization.

The artefact consists of four interconnected modules:

\subsection{Narrative Analysis of Pitch Decks}
This module uses large language models (LLMs) to analyze qualitative project materials such as pitch decks or reports.
It extracts key entities, identifies value propositions, and translates narrative inputs into structured representations.

\subsection{Semantic Similarity Search Across Frameworks}
An embedding-based search mechanism allows comparison between project narratives and reference frameworks such as the Sustainable Development Goals (SDGs) or public value dimensions.
This enables contextual mapping of activities and outcomes.

\subsection{Clustering and Thematic Grouping of Narratives}
Using vector embeddings, thematically related concepts are grouped together to reveal emergent impact patterns and shared priorities across projects.
These clusters serve as a foundation for reflection and learning rather than automated judgment.

\subsection{Automated KPI Derivation via LangGraph Pipelines}
An experimental module applies the \texttt{LangGraph} orchestration framework to derive candidate indicators and measurable outcomes from qualitative inputs.
This step illustrates how AI can support, rather than replace, expert-driven evaluation design.

\subsection{Text Analysis and Topic Modeling Pipeline}\label{subsec:text-analysis-pipeline}

To derive thematic insights and improve indicator recommendations, narrative inputs (such as problem statements, vision, and impact descriptions) are processed through a structured text analysis workflow.
This enables clustering of projects with similar focus areas and enhances automated KPI suggestions.

\begin{itemize}
    \item \textbf{Preprocessing:} Tokenization, stopword removal, and lemmatization prepare textual data for analysis.
    \item \textbf{Vectorization:} Both TF–IDF and Bag-of-Words representations are computed for interpretability.
    \item \textbf{Topic Modeling (LDA):} Latent Dirichlet Allocation identifies thematic structures within project narratives. \textbf{TODO: Train model and extract representative topics per cluster.}
    \item \textbf{Clustering:} Projects are grouped based on topic distributions or semantic embeddings to reveal recurring social and environmental domains.
    \item \textbf{Similarity Search:} Cosine similarity enables retrieval of similar projects or indicators, supporting recommendation logic.
\end{itemize}

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, rounded corners, draw=black, fill=gray!10, minimum width=9cm, minimum height=1cm, align=center},
    arrow/.style={->, thick}
]

% Nodes
\node[box] (input) {Narrative Inputs from Project Profiles};
\node[box, below=of input] (preprocess) {Text Preprocessing \\ (Tokenization, Lemmatization, Stopword Removal)};
\node[box, below=of preprocess] (vector) {Vectorization \\ (TF–IDF / Bag-of-Words)};
\node[box, below=of vector] (lda) {Topic Modeling (LDA) \\ \textbf{TODO: Extract top topics \& keywords}};
\node[box, below=of lda] (cluster) {Semantic Clustering of Projects};
\node[box, below=of cluster] (similarity) {Cosine Similarity Search on Embeddings};
\node[box, below=of similarity] (kpi) {Indicator Recommendation \\ (Top-K KPIs \& SDG Alignment)};

% Arrows
\draw[arrow] (input) -- (preprocess);
\draw[arrow] (preprocess) -- (vector);
\draw[arrow] (vector) -- (lda);
\draw[arrow] (lda) -- (cluster);
\draw[arrow] (cluster) -- (similarity);
\draw[arrow] (similarity) -- (kpi);

\end{tikzpicture}
\caption{Vertical Workflow for Text Analysis, Topic Modeling, and Indicator Recommendation}
\label{fig:text-analysis-pipeline}
\end{figure}

This pipeline not only structures unstructured text but also provides a data-driven foundation for impact assessment by identifying recurring themes and mapping them to relevant KPIs.




% ===========================
% SECTION 3.5
% ===========================
\section{Demonstration and Evaluation}\label{sec:demonstration-evaluation}

The demonstration and evaluation stages assess the artefact’s utility, usability, and relevance in its intended context.
The prototype was integrated into the digital platform of the Public Value Academy, allowing practical demonstration during workshops and learning sessions on impact and innovation.

A formative evaluation approach was adopted.
The artefact was tested with anonymized project materials and synthetic inputs to ensure data protection.
Practitioner feedback was collected through user walkthroughs and structured reflections.

Evaluation criteria included:
\begin{itemize}
    \item \textbf{Usefulness} — the extent to which AI-generated outputs supported reflection and learning,
    \item \textbf{Transparency} — the clarity of AI reasoning and output explainability,
    \item \textbf{Alignment} — consistency of generated insights with stakeholder expectations and value frameworks,
    \item \textbf{Usability} — ease of interaction and perceived integration potential within existing workflows.
\end{itemize}

Findings from the evaluation informed iterative refinement of the artefact, consistent with DSR’s cyclical nature of design, demonstration, and assessment.


% ===========================
% SECTION 3.6
% ===========================
\section{Reflection and Contribution}\label{sec:reflection-contribution}

The reflection stage consolidates theoretical and practical insights from the artefact’s design and evaluation.
From a theoretical perspective, this research extends the application of DSR into the emerging field of AI-supported impact measurement and management.
Practically, it provides a transparent, participatory, and adaptable framework for integrating AI methods into public sector innovation and learning processes.

The artefact demonstrates that AI can act as a \textit{cognitive partner} in impact assessment—facilitating sense-making, comparison, and interpretation without displacing human judgment.
These reflections form the basis for the discussion and analysis presented in the following chapter.


% ===========================
% SECTION 3.7
% ===========================
\section{Ethical Considerations}\label{sec:ethical-considerations}

Ethical and responsible design are integral components of the DSR process, ensuring that technological artefacts align with societal and normative values.
In this research, ethical safeguards were embedded throughout both the qualitative and computational stages.

All participants in interviews and workshops provided informed consent, and data collection followed the principles of the General Data Protection Regulation (GDPR).
Anonymized datasets were used for prototype testing.
From a technical perspective, explainability and transparency were prioritized by incorporating model interpretation tools such as SHAP (SHapley Additive exPlanations) and by logging all AI interactions.

Additionally, the design process considered potential risks of bias, over-automation, and the ethical use of public sector data.
Mitigation strategies included human-in-the-loop validation, traceability of model outputs, and clear boundaries between automated analysis and human interpretation.

---