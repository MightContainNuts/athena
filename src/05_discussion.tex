%! Author = deandidion
%! Date = 09.07.25

% Preamble

\chapter{Discussion}\label{ch:discussion}


\section{Interpretation of Results}\label{sec:interpretation-of-results}

The findings from this study indicate that AI has the potential to streamline and enhance impact measurement processes, especially in data-rich environments.
Large Language Models (LLMs), in particular, demonstrated capacity to synthesize qualitative feedback, extract thematic insights, and flag anomalies in reporting that might otherwise go unnoticed.
These results align with the hypothesis that AI can play a meaningful role in making social impact assessment more dynamic and responsive.

One key insight was the AI's ability to generalize across different reporting formats and extract indicators even from loosely structured narratives.
However, while this shows promise, it also raises questions about reliability and context awareness in sensitive domains.

\section{Implications for Practice}\label{sec:implications-for-practice}

If integrated thoughtfully, AI tools can reduce the burden of manual data processing and help organizations gain real-time visibility into impact.
This is particularly relevant for NGOs, foundations, and social enterprises that lack the capacity for rigorous, continuous evaluation.
AI could allow for faster feedback loops, more agile decision-making, and more inclusive participation in impact reporting — especially when language or literacy might be a barrier.

However, practitioners must be cautious.
The black-box nature of many models, potential for bias, and lack of explainability pose challenges to adoption.
Any deployment should be accompanied by human oversight and transparent documentation.

\section{Limitations}\label{sec:limitations}

The project was exploratory in nature and relied on a limited dataset of real-world but anonymized impact reports.
As such, generalizability is constrained.
Furthermore, while models like GPT-4 show strong capabilities, their outputs are probabilistic and not always consistent.
The evaluation of outputs was also subjective at times, depending on expert judgment rather than objective metrics.

Infrastructure constraints (e.g., API rate limits and costs) also restricted the scope and volume of experiments.
Future iterations should consider longitudinal deployment in live settings.

\section{Comparison with Related Work}\label{sec:comparison-with-related-work}

Compared to prior efforts in automated evaluation, this study focused less on metrics and more on narrative insight extraction.
While previous research often centers on quantifying outputs or outcomes using structured data, this work contributes to a growing body of research exploring the use of AI for *qualitative* and *contextual* understanding.

The approach differs from dashboard-based monitoring systems by aiming to reduce the manual step of transforming stories and observations into impact insights.

\section{Ethical and Social Considerations}\label{sec:ethical-and-social-considerations}

AI's use in social impact work must be held to high ethical standards.
The risk of algorithmic bias — especially when assessing outcomes for marginalized communities — is non-trivial.
AI systems can unintentionally reinforce dominant narratives or overlook voices that do not fit existing training patterns.

Data privacy is another concern.
Even anonymized reports can carry sensitive context that AI models may not treat with the nuance required.
Moreover, the use of AI to interpret human stories raises deeper philosophical questions about whose perspective is prioritized in the act of measurement.

Any use of AI in this space must be accountable, auditable, and designed with input from affected stakeholders.
